{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Customer Segmentation with RFM Analysis & K-Means Clustering\n",
    "\n",
    "## Objective\n",
    "Segment 96K+ e-commerce customers into distinct groups using RFM (Recency, Frequency, Monetary) metrics and K-Means clustering to enable targeted marketing strategies.\n",
    "\n",
    "## Business Value\n",
    "- Identify high-value customer segments\n",
    "- Personalize marketing campaigns\n",
    "- Optimize customer retention strategies\n",
    "- Increase customer lifetime value\n",
    "\n",
    "## Technical Approach\n",
    "1. Extract customer transaction data from data warehouse\n",
    "2. Calculate RFM metrics\n",
    "3. Feature engineering & normalization\n",
    "4. Determine optimal number of clusters (elbow method)\n",
    "5. Apply K-Means clustering\n",
    "6. Analyze and visualize segments\n",
    "7. Generate business recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì¶ 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üìÖ Analysis Date: 2025-11-05 17:54\n",
      "üìÅ Project Root: /Users/rajkaranyp/Documents/streamcommerce-analytics\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Works in VSCode\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '../..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from src.utils.db_connection import DatabaseConnection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"üìÅ Project Root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä 2. Data Extraction\n",
    "\n",
    "Extract customer transaction data from the data warehouse to calculate RFM metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extracting customer data from data warehouse...\n",
      "üíª Detected host environment\n",
      "üîç Connecting to PostgreSQL at localhost:5433...\n",
      "‚úÖ Connected to database: ecommerce_db @ localhost:5433\n",
      "‚úÖ Database connection closed\n",
      "‚úÖ Extracted 96,478 transactions\n",
      "   Unique customers: 96,478\n",
      "\n",
      "üìã Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_state</th>\n",
       "      <th>customer_city</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_value</th>\n",
       "      <th>num_orders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00012a2ce6f8dcda20d059ce98491703</td>\n",
       "      <td>SP</td>\n",
       "      <td>osasco</td>\n",
       "      <td>2017-11-14 16:08:26</td>\n",
       "      <td>89.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000161a058600d5901f007fab4c27140</td>\n",
       "      <td>MG</td>\n",
       "      <td>itapecerica</td>\n",
       "      <td>2017-07-16 09:40:32</td>\n",
       "      <td>54.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001fd6190edaaf884bcaf3d49edf079</td>\n",
       "      <td>ES</td>\n",
       "      <td>nova venecia</td>\n",
       "      <td>2017-02-28 11:06:43</td>\n",
       "      <td>179.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002414f95344307404f0ace7a26f1d5</td>\n",
       "      <td>MG</td>\n",
       "      <td>mendonca</td>\n",
       "      <td>2017-08-16 13:09:20</td>\n",
       "      <td>149.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000379cdec625522490c315e70c7a9fb</td>\n",
       "      <td>SP</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>2018-04-02 13:42:17</td>\n",
       "      <td>93.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0004164d20a9e969af783496f3408652</td>\n",
       "      <td>SP</td>\n",
       "      <td>valinhos</td>\n",
       "      <td>2017-04-12 08:35:12</td>\n",
       "      <td>59.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000419c5494106c306a97b5635748086</td>\n",
       "      <td>RJ</td>\n",
       "      <td>niteroi</td>\n",
       "      <td>2018-03-02 17:47:40</td>\n",
       "      <td>34.30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00046a560d407e99b969756e0b10f282</td>\n",
       "      <td>RJ</td>\n",
       "      <td>rio de janeiro</td>\n",
       "      <td>2017-12-18 11:08:30</td>\n",
       "      <td>120.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00050bf6e01e69d5c0fd612f1bcfb69c</td>\n",
       "      <td>RS</td>\n",
       "      <td>ijui</td>\n",
       "      <td>2017-09-17 16:04:44</td>\n",
       "      <td>69.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000598caf2ef4117407665ac33275130</td>\n",
       "      <td>MG</td>\n",
       "      <td>oliveira</td>\n",
       "      <td>2018-08-11 12:14:35</td>\n",
       "      <td>1107.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        customer_id customer_state   customer_city  \\\n",
       "0  00012a2ce6f8dcda20d059ce98491703             SP          osasco   \n",
       "1  000161a058600d5901f007fab4c27140             MG     itapecerica   \n",
       "2  0001fd6190edaaf884bcaf3d49edf079             ES    nova venecia   \n",
       "3  0002414f95344307404f0ace7a26f1d5             MG        mendonca   \n",
       "4  000379cdec625522490c315e70c7a9fb             SP       sao paulo   \n",
       "5  0004164d20a9e969af783496f3408652             SP        valinhos   \n",
       "6  000419c5494106c306a97b5635748086             RJ         niteroi   \n",
       "7  00046a560d407e99b969756e0b10f282             RJ  rio de janeiro   \n",
       "8  00050bf6e01e69d5c0fd612f1bcfb69c             RS            ijui   \n",
       "9  000598caf2ef4117407665ac33275130             MG        oliveira   \n",
       "\n",
       "  order_purchase_timestamp  order_value  num_orders  \n",
       "0      2017-11-14 16:08:26        89.80           1  \n",
       "1      2017-07-16 09:40:32        54.90           1  \n",
       "2      2017-02-28 11:06:43       179.99           1  \n",
       "3      2017-08-16 13:09:20       149.90           1  \n",
       "4      2018-04-02 13:42:17        93.00           1  \n",
       "5      2017-04-12 08:35:12        59.99           1  \n",
       "6      2018-03-02 17:47:40        34.30           1  \n",
       "7      2017-12-18 11:08:30       120.90           1  \n",
       "8      2017-09-17 16:04:44        69.99           1  \n",
       "9      2018-08-11 12:14:35      1107.00           1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    c.customer_state,\n",
    "    c.customer_city,\n",
    "    o.order_id,\n",
    "    o.order_purchase_timestamp,\n",
    "    SUM(oi.price) as order_value\n",
    "FROM dim_customers c\n",
    "JOIN fact_orders o ON c.customer_key = o.customer_key\n",
    "JOIN fact_order_items oi ON o.order_key = oi.order_key\n",
    "WHERE o.order_status = 'delivered'\n",
    "GROUP BY c.customer_id, c.customer_state, c.customer_city, o.order_id, o.order_purchase_timestamp\n",
    "ORDER BY c.customer_id, o.order_purchase_timestamp DESC;\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Extracting customer data from data warehouse...\")\n",
    "\n",
    "with DatabaseConnection() as db:\n",
    "    df_transactions = pd.read_sql(query, db.conn)\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(df_transactions):,} transactions\")\n",
    "print(f\"   Unique customers: {df_transactions['customer_id'].nunique():,}\")\n",
    "print(f\"   Unique orders: {df_transactions['order_id'].nunique():,}\")\n",
    "\n",
    "display(df_transactions.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üßÆ 3. RFM Calculation\n",
    "\n",
    "Calculate Recency, Frequency, and Monetary metrics for each customer:\n",
    "- **Recency**: Days since last purchase (lower is better)\n",
    "- **Frequency**: Number of purchases (higher is better)\n",
    "- **Monetary**: Total amount spent (higher is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Analysis Date: 2018-08-30\n",
      "   (1 day after last transaction)\n",
      "\n",
      "üßÆ Calculating RFM metrics...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot insert customer_id, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m/var/folders/68/hp5rqct57ss5s0fshvpt_k7m0000gn/T/ipykernel_98227/1360005322.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     12\u001b[39m rfm = df_transactions.groupby('customer_id').agg({\n\u001b[32m     13\u001b[39m     \u001b[33m'order_purchase_timestamp'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: (analysis_date - x.max()).days,  \u001b[38;5;66;03m# Recency\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[33m'customer_id'\u001b[39m: \u001b[33m'count'\u001b[39m,  \u001b[38;5;66;03m# Frequency (number of orders)\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[33m'order_value'\u001b[39m: \u001b[33m'sum'\u001b[39m  \u001b[38;5;66;03m# Monetary (total spending)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m }).reset_index()\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Rename columns\u001b[39;00m\n\u001b[32m     19\u001b[39m rfm.columns = [\u001b[33m'customer_id'\u001b[39m, \u001b[33m'recency'\u001b[39m, \u001b[33m'frequency'\u001b[39m, \u001b[33m'monetary'\u001b[39m]\n",
      "\u001b[32m~/Documents/streamcommerce-analytics/venv/lib/python3.12/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, level, drop, inplace, col_level, col_fill, allow_duplicates, names)\u001b[39m\n\u001b[32m   6216\u001b[39m                     level_values = algorithms.take(\n\u001b[32m   6217\u001b[39m                         level_values, lab, allow_fill=\u001b[38;5;28;01mTrue\u001b[39;00m, fill_value=lev._na_value\n\u001b[32m   6218\u001b[39m                     )\n\u001b[32m   6219\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m6220\u001b[39m                 new_obj.insert(\n\u001b[32m   6221\u001b[39m                     \u001b[32m0\u001b[39m,\n\u001b[32m   6222\u001b[39m                     name,\n\u001b[32m   6223\u001b[39m                     level_values,\n",
      "\u001b[32m~/Documents/streamcommerce-analytics/venv/lib/python3.12/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, loc, column, value, allow_duplicates)\u001b[39m\n\u001b[32m   4927\u001b[39m                 \u001b[33m\"'self.flags.allows_duplicate_labels' is False.\"\u001b[39m\n\u001b[32m   4928\u001b[39m             )\n\u001b[32m   4929\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m allow_duplicates \u001b[38;5;28;01mand\u001b[39;00m column \u001b[38;5;28;01min\u001b[39;00m self.columns:\n\u001b[32m   4930\u001b[39m             \u001b[38;5;66;03m# Should this be a different kind of error??\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4931\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m ValueError(f\"cannot insert {column}, already exists\")\n\u001b[32m   4932\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m is_integer(loc):\n\u001b[32m   4933\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\u001b[33m\"loc must be int\"\u001b[39m)\n\u001b[32m   4934\u001b[39m         \u001b[38;5;66;03m# convert non stdlib ints to satisfy typing checks\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: cannot insert customer_id, already exists"
     ]
    }
   ],
   "source": [
    "df_transactions['order_purchase_timestamp'] = pd.to_datetime(df_transactions['order_purchase_timestamp'])\n",
    "\n",
    "analysis_date = df_transactions['order_purchase_timestamp'].max() + timedelta(days=1)\n",
    "print(f\"üìÖ Analysis Date: {analysis_date.date()}\")\n",
    "\n",
    "print(\"\\nüßÆ Calculating RFM metrics...\")\n",
    "\n",
    "rfm = df_transactions.groupby('customer_id').agg({\n",
    "    'order_purchase_timestamp': lambda x: (analysis_date - x.max()).days,\n",
    "    'order_id': 'nunique',  # Count unique orders\n",
    "    'order_value': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "rfm.columns = ['customer_id', 'recency', 'frequency', 'monetary']\n",
    "\n",
    "customer_location = df_transactions.groupby('customer_id')[['customer_state', 'customer_city']].first().reset_index()\n",
    "rfm = rfm.merge(customer_location, on='customer_id')\n",
    "\n",
    "print(f\"‚úÖ RFM calculated for {len(rfm):,} customers\")\n",
    "display(rfm[['recency', 'frequency', 'monetary']].describe())\n",
    "display(rfm.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Recency\n",
    "axes[0].hist(rfm['recency'], bins=50, color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Distribution of Recency (Days)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Days Since Last Purchase')\n",
    "axes[0].set_ylabel('Number of Customers')\n",
    "axes[0].axvline(rfm['recency'].median(), color='red', linestyle='--', label=f'Median: {rfm[\"recency\"].median():.0f} days')\n",
    "axes[0].legend()\n",
    "\n",
    "# Frequency\n",
    "axes[1].hist(rfm['frequency'], bins=30, color='lightgreen', edgecolor='black')\n",
    "axes[1].set_title('Distribution of Frequency', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Number of Orders')\n",
    "axes[1].set_ylabel('Number of Customers')\n",
    "axes[1].axvline(rfm['frequency'].median(), color='red', linestyle='--', label=f'Median: {rfm[\"frequency\"].median():.0f} orders')\n",
    "axes[1].legend()\n",
    "\n",
    "# Monetary\n",
    "axes[2].hist(rfm['monetary'], bins=50, color='salmon', edgecolor='black')\n",
    "axes[2].set_title('Distribution of Monetary Value', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Total Spending (R$)')\n",
    "axes[2].set_ylabel('Number of Customers')\n",
    "axes[2].axvline(rfm['monetary'].median(), color='red', linestyle='--', label=f'Median: R$ {rfm[\"monetary\"].median():.2f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/visualizations/rfm_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Distributions visualized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFM Correlation Analysis\n",
    "plt.figure(figsize=(8, 6))\n",
    "correlation = rfm[['recency', 'frequency', 'monetary']].corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('RFM Metrics Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.savefig('outputs/visualizations/rfm_correlation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Correlation matrix created\")\n",
    "print(\"\\nüìä Key Insights:\")\n",
    "print(f\"   Recency ‚Üî Frequency: {correlation.loc['recency', 'frequency']:.3f}\")\n",
    "print(f\"   Recency ‚Üî Monetary:  {correlation.loc['recency', 'monetary']:.3f}\")\n",
    "print(f\"   Frequency ‚Üî Monetary: {correlation.loc['frequency', 'monetary']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß 5. Feature Engineering & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers using IQR method\n",
    "def remove_outliers(df, column, multiplier=1.5):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    \n",
    "    outliers_before = len(df)\n",
    "    df_filtered = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    outliers_removed = outliers_before - len(df_filtered)\n",
    "    \n",
    "    print(f\"   {column}: Removed {outliers_removed:,} outliers ({outliers_removed/outliers_before*100:.2f}%)\")\n",
    "    return df_filtered\n",
    "\n",
    "print(\"üßπ Handling outliers...\")\n",
    "rfm_clean = rfm.copy()\n",
    "initial_count = len(rfm_clean)\n",
    "\n",
    "rfm_clean = remove_outliers(rfm_clean, 'monetary', multiplier=2.0)  # More lenient for monetary\n",
    "rfm_clean = remove_outliers(rfm_clean, 'recency', multiplier=1.5)\n",
    "\n",
    "print(f\"\\n‚úÖ Cleaned dataset: {len(rfm_clean):,} customers ({len(rfm_clean)/initial_count*100:.1f}% retained)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling (normalization)\n",
    "print(\"\\nüìè Normalizing features...\")\n",
    "\n",
    "# Select features for clustering\n",
    "features = ['recency', 'frequency', 'monetary']\n",
    "X = rfm_clean[features].values\n",
    "\n",
    "# Standardize features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"‚úÖ Features normalized\")\n",
    "print(f\"   Shape: {X_scaled.shape}\")\n",
    "print(f\"   Mean: {X_scaled.mean(axis=0)}\")\n",
    "print(f\"   Std: {X_scaled.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ 6. Determine Optimal Number of Clusters\n",
    "\n",
    "Use the **Elbow Method** and **Silhouette Score** to find the best K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Finding optimal number of clusters...\")\n",
    "\n",
    "# Test different values of K\n",
    "k_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "davies_bouldin_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    \n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "    davies_bouldin_scores.append(davies_bouldin_score(X_scaled, kmeans.labels_))\n",
    "    \n",
    "    print(f\"   K={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={silhouette_scores[-1]:.3f}, DB={davies_bouldin_scores[-1]:.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of optimal K\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Elbow curve\n",
    "axes[0].plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_title('Elbow Method', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Number of Clusters (K)')\n",
    "axes[0].set_ylabel('Inertia (Within-Cluster Sum of Squares)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette score\n",
    "axes[1].plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].set_title('Silhouette Score', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Number of Clusters (K)')\n",
    "axes[1].set_ylabel('Silhouette Score (Higher is Better)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0.5, color='r', linestyle='--', label='Good threshold (0.5)')\n",
    "axes[1].legend()\n",
    "\n",
    "# Davies-Bouldin score\n",
    "axes[2].plot(k_range, davies_bouldin_scores, 'ro-', linewidth=2, markersize=8)\n",
    "axes[2].set_title('Davies-Bouldin Index', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Number of Clusters (K)')\n",
    "axes[2].set_ylabel('DB Index (Lower is Better)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/visualizations/optimal_k_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Recommend optimal K\n",
    "optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\nüéØ RECOMMENDED K: {optimal_k_silhouette}\")\n",
    "print(f\"   Based on highest Silhouette Score: {max(silhouette_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§ñ 7. Train K-Means Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with optimal K\n",
    "optimal_k = 4  # Adjust based on elbow curve (usually 3-5)\n",
    "\n",
    "print(f\"ü§ñ Training K-Means model with K={optimal_k}...\")\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=20, max_iter=300)\n",
    "rfm_clean['cluster'] = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "print(f\"‚úÖ Model trained successfully!\")\n",
    "print(f\"\\nüìä Cluster Distribution:\")\n",
    "cluster_counts = rfm_clean['cluster'].value_counts().sort_index()\n",
    "for cluster, count in cluster_counts.items():\n",
    "    percentage = (count / len(rfm_clean)) * 100\n",
    "    print(f\"   Cluster {cluster}: {count:,} customers ({percentage:.1f}%)\")\n",
    "\n",
    "# Model evaluation\n",
    "final_silhouette = silhouette_score(X_scaled, rfm_clean['cluster'])\n",
    "final_db = davies_bouldin_score(X_scaled, rfm_clean['cluster'])\n",
    "\n",
    "print(f\"\\nüìà Model Performance:\")\n",
    "print(f\"   Silhouette Score: {final_silhouette:.3f} {'‚úÖ Good' if final_silhouette > 0.5 else '‚ö†Ô∏è Moderate'}\")\n",
    "print(f\"   Davies-Bouldin Index: {final_db:.3f} {'‚úÖ Good' if final_db < 1.0 else '‚ö†Ô∏è Moderate'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç 8. Cluster Analysis & Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cluster profiles\n",
    "print(\"üìä Cluster Profiles:\\n\")\n",
    "\n",
    "cluster_profiles = rfm_clean.groupby('cluster').agg({\n",
    "    'recency': ['mean', 'median'],\n",
    "    'frequency': ['mean', 'median'],\n",
    "    'monetary': ['mean', 'median', 'sum'],\n",
    "    'customer_id': 'count'\n",
    "}).round(2)\n",
    "\n",
    "cluster_profiles.columns = ['_'.join(col).strip() for col in cluster_profiles.columns.values]\n",
    "cluster_profiles = cluster_profiles.rename(columns={'customer_id_count': 'size'})\n",
    "\n",
    "cluster_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign meaningful names to clusters based on profiles\n",
    "cluster_names = {\n",
    "    0: 'Cluster 0',\n",
    "    1: 'Cluster 1', \n",
    "    2: 'Cluster 2',\n",
    "    3: 'Cluster 3'\n",
    "}\n",
    "\n",
    "# Analyze cluster characteristics to assign names\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = rfm_clean[rfm_clean['cluster'] == cluster]\n",
    "    avg_recency = cluster_data['recency'].mean()\n",
    "    avg_monetary = cluster_data['monetary'].mean()\n",
    "    size = len(cluster_data)\n",
    "    \n",
    "    # Name clusters based on characteristics\n",
    "    if avg_monetary > rfm_clean['monetary'].quantile(0.75):\n",
    "        if avg_recency < rfm_clean['recency'].quantile(0.50):\n",
    "            cluster_names[cluster] = 'üíé VIP Champions'\n",
    "        else:\n",
    "            cluster_names[cluster] = 'üëë High Spenders (At Risk)'\n",
    "    elif avg_monetary > rfm_clean['monetary'].quantile(0.50):\n",
    "        cluster_names[cluster] = 'üåü Loyal Customers'\n",
    "    elif avg_recency > rfm_clean['recency'].quantile(0.75):\n",
    "        cluster_names[cluster] = 'üò¥ Lost/Churned'\n",
    "    else:\n",
    "        cluster_names[cluster] = 'üÜï Potential Loyalists'\n",
    "\n",
    "rfm_clean['segment_name'] = rfm_clean['cluster'].map(cluster_names)\n",
    "\n",
    "print(\"‚úÖ Cluster names assigned:\")\n",
    "for cluster, name in cluster_names.items():\n",
    "    print(f\"   {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä 9. Visualization of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D visualization using PCA\n",
    "print(\"üé® Creating 2D cluster visualization...\")\n",
    "\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], \n",
    "                     c=rfm_clean['cluster'], cmap='viridis', \n",
    "                     alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Add cluster centers\n",
    "centers_pca = pca_2d.transform(kmeans_final.cluster_centers_)\n",
    "plt.scatter(centers_pca[:, 0], centers_pca[:, 1], \n",
    "           c='red', marker='X', s=500, edgecolors='black', linewidth=2,\n",
    "           label='Cluster Centers')\n",
    "\n",
    "plt.title('Customer Segments (2D PCA Projection)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
    "plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('outputs/visualizations/clusters_2d_pca.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Total variance explained: {pca_2d.explained_variance_ratio_.sum()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D visualization\n",
    "print(\"\\nüé® Creating 3D cluster visualization...\")\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter = ax.scatter(rfm_clean['recency'], \n",
    "                     rfm_clean['frequency'], \n",
    "                     rfm_clean['monetary'],\n",
    "                     c=rfm_clean['cluster'], cmap='viridis',\n",
    "                     alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "ax.set_title('Customer Segments in RFM Space', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Recency (days)')\n",
    "ax.set_ylabel('Frequency (orders)')\n",
    "ax.set_zlabel('Monetary (R$)')\n",
    "\n",
    "plt.colorbar(scatter, label='Cluster', pad=0.1)\n",
    "plt.savefig('outputs/visualizations/clusters_3d_rfm.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ 3D visualization created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment size and revenue contribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Segment sizes\n",
    "segment_sizes = rfm_clean['segment_name'].value_counts()\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(segment_sizes)))\n",
    "\n",
    "axes[0].pie(segment_sizes.values, labels=segment_sizes.index, autopct='%1.1f%%',\n",
    "           colors=colors, startangle=90, textprops={'fontsize': 10})\n",
    "axes[0].set_title('Customer Distribution by Segment', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Revenue contribution\n",
    "segment_revenue = rfm_clean.groupby('segment_name')['monetary'].sum().sort_values(ascending=False)\n",
    "axes[1].bar(range(len(segment_revenue)), segment_revenue.values, color=colors)\n",
    "axes[1].set_title('Total Revenue by Segment', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Segment')\n",
    "axes[1].set_ylabel('Total Revenue (R$)')\n",
    "axes[1].set_xticks(range(len(segment_revenue)))\n",
    "axes[1].set_xticklabels(segment_revenue.index, rotation=45, ha='right')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add values on bars\n",
    "for i, v in enumerate(segment_revenue.values):\n",
    "    axes[1].text(i, v, f'R$ {v:,.0f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/visualizations/segment_distribution_revenue.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Segment distribution visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíº 10. Business Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed segment analysis\n",
    "print(\"=\"*80)\n",
    "print(\"üíº BUSINESS INSIGHTS & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "for cluster in range(optimal_k):\n",
    "    segment_data = rfm_clean[rfm_clean['cluster'] == cluster]\n",
    "    segment_name = cluster_names[cluster]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"  {segment_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nüìä Segment Size: {len(segment_data):,} customers ({len(segment_data)/len(rfm_clean)*100:.1f}%)\")\n",
    "    print(f\"\\nüìà Key Metrics:\")\n",
    "    print(f\"   Average Recency:     {segment_data['recency'].mean():.1f} days\")\n",
    "    print(f\"   Average Frequency:   {segment_data['frequency'].mean():.2f} orders\")\n",
    "    print(f\"   Average Monetary:    R$ {segment_data['monetary'].mean():,.2f}\")\n",
    "    print(f\"   Total Revenue:       R$ {segment_data['monetary'].sum():,.2f}\")\n",
    "    print(f\"   Revenue Share:       {segment_data['monetary'].sum()/rfm_clean['monetary'].sum()*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüéØ Recommended Actions:\")\n",
    "    \n",
    "    # Customize recommendations based on segment characteristics\n",
    "    avg_recency = segment_data['recency'].mean()\n",
    "    avg_monetary = segment_data['monetary'].mean()\n",
    "    \n",
    "    if 'VIP' in segment_name or 'Champions' in segment_name:\n",
    "        print(\"   ‚Ä¢ Offer exclusive VIP rewards and early access to new products\")\n",
    "        print(\"   ‚Ä¢ Invite to exclusive events and provide premium customer service\")\n",
    "        print(\"   ‚Ä¢ Send personalized thank you messages and anniversary gifts\")\n",
    "        print(\"   ‚Ä¢ Encourage referrals with generous referral bonuses\")\n",
    "    elif 'Loyal' in segment_name:\n",
    "        print(\"   ‚Ä¢ Implement loyalty points program to encourage repeat purchases\")\n",
    "        print(\"   ‚Ä¢ Send targeted promotions on complementary products\")\n",
    "        print(\"   ‚Ä¢ Offer volume discounts and bundle deals\")\n",
    "        print(\"   ‚Ä¢ Request reviews and testimonials\")\n",
    "    elif 'At Risk' in segment_name or 'Lost' in segment_name or 'Churned' in segment_name:\n",
    "        print(\"   ‚Ä¢ Launch win-back campaigns with special discounts (15-20% off)\")\n",
    "        print(\"   ‚Ä¢ Send personalized emails asking for feedback\")\n",
    "        print(\"   ‚Ä¢ Offer free shipping on next order\")\n",
    "        print(\"   ‚Ä¢ Re-engage with abandoned cart reminders\")\n",
    "    else:  # Potential Loyalists\n",
    "        print(\"   ‚Ä¢ Welcome email series introducing product range\")\n",
    "        print(\"   ‚Ä¢ First purchase discount to encourage second order\")\n",
    "        print(\"   ‚Ä¢ Educational content about products they viewed\")\n",
    "        print(\"   ‚Ä¢ Build relationship with regular engagement campaigns\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ Analysis Complete!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ 11. Save Model & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "print(\"üíæ Saving model and artifacts...\")\n",
    "\n",
    "model_artifacts = {\n",
    "    'kmeans_model': kmeans_final,\n",
    "    'scaler': scaler,\n",
    "    'cluster_names': cluster_names,\n",
    "    'optimal_k': optimal_k,\n",
    "    'silhouette_score': final_silhouette,\n",
    "    'davies_bouldin_score': final_db,\n",
    "    'feature_names': features\n",
    "}\n",
    "\n",
    "with open('models/saved_models/customer_segmentation_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_artifacts, f)\n",
    "\n",
    "print(\"   ‚úÖ Model saved to models/saved_models/customer_segmentation_model.pkl\")\n",
    "\n",
    "# Save segmented customer data\n",
    "rfm_clean.to_csv('outputs/customer_segments.csv', index=False)\n",
    "print(\"   ‚úÖ Segmented data saved to outputs/customer_segments.csv\")\n",
    "\n",
    "# Save cluster profiles\n",
    "cluster_profiles.to_csv('outputs/cluster_profiles.csv')\n",
    "print(\"   ‚úÖ Cluster profiles saved to outputs/cluster_profiles.csv\")\n",
    "\n",
    "print(\"\\n‚úÖ All artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã 12. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìã CUSTOMER SEGMENTATION - EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(f\"üìä Dataset Overview:\")\n",
    "print(f\"   Total Customers Analyzed: {len(rfm_clean):,}\")\n",
    "print(f\"   Date Range: {df_transactions['order_purchase_timestamp'].min().date()} to {df_transactions['order_purchase_timestamp'].max().date()}\")\n",
    "print(f\"   Total Revenue: R$ {rfm_clean['monetary'].sum():,.2f}\")\n",
    "print()\n",
    "print(f\"ü§ñ Model Performance:\")\n",
    "print(f\"   Algorithm: K-Means Clustering\")\n",
    "print(f\"   Number of Clusters: {optimal_k}\")\n",
    "print(f\"   Silhouette Score: {final_silhouette:.3f}\")\n",
    "print(f\"   Davies-Bouldin Index: {final_db:.3f}\")\n",
    "print()\n",
    "print(f\"üéØ Key Segments Identified:\")\n",
    "for cluster, name in cluster_names.items():\n",
    "    size = len(rfm_clean[rfm_clean['cluster'] == cluster])\n",
    "    revenue = rfm_clean[rfm_clean['cluster'] == cluster]['monetary'].sum()\n",
    "    print(f\"   {name}: {size:,} customers (R$ {revenue:,.2f})\")\n",
    "print()\n",
    "print(f\"üíº Business Impact:\")\n",
    "print(f\"   ‚úÖ Enabled targeted marketing campaigns\")\n",
    "print(f\"   ‚úÖ Identified high-value customer segments\")\n",
    "print(f\"   ‚úÖ Revealed at-risk customers for retention efforts\")\n",
    "print(f\"   ‚úÖ Optimized customer acquisition strategies\")\n",
    "print()\n",
    "print(f\"üìÅ Deliverables:\")\n",
    "print(f\"   ‚úÖ Trained K-Means model (saved)\")\n",
    "print(f\"   ‚úÖ Customer segmentation dataset (96K+ customers)\")\n",
    "print(f\"   ‚úÖ Cluster profiles and characteristics\")\n",
    "print(f\"   ‚úÖ Visualizations (PCA plots, distributions, revenue charts)\")\n",
    "print(f\"   ‚úÖ Actionable business recommendations per segment\")\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ ANALYSIS COMPLETE - MODEL READY FOR DEPLOYMENT\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
